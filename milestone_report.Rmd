---
title: "Milestone Report"
author: "Susana Arias Laso"
date: '2020-03-11'
output: html_document
---

```{r include=FALSE, results = 'hide', message = FALSE, warning = FALSE}
library(tm)
library(hash)
library(ggplot2)
library(dplyr)
library(knitr)


load_text <- function(dir) {
    
    textData <- VCorpus(DirSource(directory = dir))
    
    textData <- tm_map(textData, removePunctuation)
    textData <- tm_map(textData, removeNumbers)
    textData <- tm_map(textData, removeWords, stopwords("SMART"))
    textData <- tm_map(textData, content_transformer(tolower))
    
    # remove all the characters that are not letters or numbers
    toSpace <- content_transformer(function(x, pattern) {
        return (gsub(pattern, " ", x))
    })
    textData <- tm_map(textData, toSpace, "[^a-z']")
    textData <- tm_map(textData, stripWhitespace)
    
    return(textData)
}
```


## Introduction

This milestone report shows some general features about the data that will be 
used in the prediction model. The corpus consists of English documents where 
the text is extracted from news, blogs and Twitter. A fraction of 1% of the 
corpus has been selected randomly to do some exploratory analysis.

```{r, echo = FALSE}
dir_corpus <- "/Users/sariaslaso/Google Drive/courses/data science specialization/capstone project/week2"

corpus <- load_text(paste(dir_corpus, "corpus", sep = "/")) 
blogs <- as.character(corpus[[1]])
news <- as.character(corpus[[2]])
twitter <- as.character(corpus[[3]])
```

## Summary Statistics

```{r, echo=FALSE, results='hide'}
tokenize_text <- function(doc) {
    f <- function(vec) {return(strsplit(vec, " "))}
    doc_split <- sapply(doc, f)
    
    return(doc_split)
}



count_ngrams <- function(tokenized_text, n) {
    f <- function(list) {return(ngrams(list, n))}
    text_ngrams <- sapply(tokenized_text, f)
    
    ngram_dictionary <- hash()
    
    for(doc_index in 1:length(text_ngrams)) {
        if (doc_index %% 1000 == 0) {
            cat(sprintf("%d/%d\n", doc_index, length(text_ngrams)))
        }
        for (ngram in text_ngrams[[doc_index]]) {
            if (prod(nchar(ngram)) > 0) {
                key <- paste(ngram, collapse = " ")
                if (has.key(key, ngram_dictionary)) {
                    ngram_dictionary[[key]] = ngram_dictionary[[key]] + 1
                } else {
                    ngram_dictionary[[key]] = 1
                }
                
            } 
        }
    }
    return(ngram_dictionary)
    
}

to_df <- function(ngram) {
    df <- data.frame(keys = names(ngram), values = values(ngram), 
                     row.names = NULL)
    return(df)
}

token_blogs <- tokenize_text(blogs)
token_news <- tokenize_text(news)
token_twitter <- tokenize_text(twitter)

unigram_blog <- count_ngrams(token_blogs, n=1)
unigram_news <- count_ngrams(token_news, n=1)
unigram_twitter <- count_ngrams(token_twitter, n=1)

bigram_blog <- count_ngrams(token_blogs, n=2)
bigram_news <- count_ngrams(token_news, n=2)
bigram_twitter <- count_ngrams(token_twitter, n=2)

unigram_blog_df <- to_df(unigram_blog)
unigram_news_df <- to_df(unigram_news)
unigram_twitter_df <- to_df(unigram_twitter)

bigram_blog_df <- to_df(bigram_blog)
bigram_news_df <- to_df(bigram_news)
bigram_twitter_df <- to_df(bigram_twitter)
```

```{r, include=FALSE}
row_names <- c("blog", "news", "twitter")
unigram_summary <- sapply(list(unigram_blog_df$values, unigram_news_df$values, 
                               unigram_twitter_df$values), summary)
bigram_summary <- sapply(list(bigram_blog_df$values, bigram_news_df$values, 
                               bigram_twitter_df$values), summary)
unigram_summary <- t(unigram_summary)
bigram_summary <- t(bigram_summary)
row.names(unigram_summary) <- row_names
row.names(bigram_summary) <- row_names
```

The following tables summarize the distribution of unigrams and bigrams in the 
reduced portion of the original corpus, respectively.
```{r, echo=FALSE}
kable(unigram_summary, caption = "unigrams")
kable(bigram_summary, caption = "bigrams")
```

The ten most frequent 1-grams and 2-grams were selected from the data and are 
shown in the following bar plots for each source of text. These frequencies 
correspond to 1% of each file.

```{r, include=FALSE, results='hide'}
ngram_df <- function(df, n) {
    top10_ngram_df <- df %>%
        top_n(10, values) %>%
        mutate(keys = sapply(keys, toString)) %>%
        mutate(ngram = n) %>%
        mutate(freq = values / sum(df$values) * 100)
    
    return(top10_ngram_df)
}

unigram_blog <- ngram_df(unigram_blog_df, 1) %>%
    mutate(source = "blogs")
unigram_news <- ngram_df(unigram_news_df, 1) %>% 
    mutate(source = "news")
unigram_twitter <- ngram_df(unigram_twitter_df, 1) %>%
    mutate(source = "twitter")

bigram_blog <- ngram_df(bigram_blog_df, 2) %>%
    mutate(source = "blogs")
bigram_news <- ngram_df(bigram_news_df, 2) %>%
    mutate(source = "news")
bigram_twitter <- ngram_df(bigram_twitter_df, 2) %>%
    mutate(source = "twitter")

unigrams <- rbind(unigram_blog, unigram_news, unigram_twitter)
bigrams <- rbind(bigram_blog, bigram_news, bigram_twitter)
```

```{r, include=FALSE}
plot_ngrams <- function(ngram) {
    ggplot(ngram, aes(x = factor(keys), y = freq)) + 
        geom_bar(stat = "identity") + 
        facet_grid(cols = vars(factor(source)), scales = "free") + 
        coord_flip() + 
        xlab("n-grams") + ylab("freq (%)")
}
```

```{r, echo=FALSE}
plot_ngrams(unigrams)
```

```{r, echo=FALSE}
plot_ngrams(bigrams)
```

## Conclusions and Prediction Algorithm

A preliminary view of the data indicates that with the text being in English
some common words such as "I" and "the" seem to have a high frequency of use.
Similar features become clear as well, such as the language and common bigrams
used in news, e.g., "new york", "high school" and "los angeles". Similarly,
tweets in first person are more frequent than more impersonal phrases. The 
original corpus of English texts will be split into training and test sets and
a prediction algorithm based on conditional probabilities of the n-grams will
be implemented.











