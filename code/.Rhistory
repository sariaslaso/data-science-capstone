library(tm)
library(hash)
library(ggplot2)
library(dplyr)
library(knitr)
load_text <- function(dir) {
textData <- VCorpus(DirSource(directory = dir))
textData <- tm_map(textData, removePunctuation)
textData <- tm_map(textData, removeNumbers)
textData <- tm_map(textData, removeWords, stopwords("SMART"))
textData <- tm_map(textData, content_transformer(tolower))
# remove all the characters that are not letters or numbers
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
})
textData <- tm_map(textData, toSpace, "[^a-z']")
textData <- tm_map(textData, stripWhitespace)
return(textData)
}
dir_corpus <- "/Users/sariaslaso/Google Drive/courses/data science specialization/capstone project/week2"
corpus <- load_text(paste(dir_corpus, "corpus", sep = "/"))
blogs <- as.character(corpus[[1]])
news <- as.character(corpus[[2]])
twitter <- as.character(corpus[[3]])
tokenize_text <- function(doc) {
f <- function(vec) {return(strsplit(vec, " "))}
doc_split <- sapply(doc, f)
return(doc_split)
}
count_ngrams <- function(tokenized_text, n) {
f <- function(list) {return(ngrams(list, n))}
text_ngrams <- sapply(tokenized_text, f)
ngram_dictionary <- hash()
for(doc_index in 1:length(text_ngrams)) {
if (doc_index %% 1000 == 0) {
cat(sprintf("%d/%d\n", doc_index, length(text_ngrams)))
}
for (ngram in text_ngrams[[doc_index]]) {
if (prod(nchar(ngram)) > 0) {
key <- paste(ngram, collapse = " ")
if (has.key(key, ngram_dictionary)) {
ngram_dictionary[[key]] = ngram_dictionary[[key]] + 1
} else {
ngram_dictionary[[key]] = 1
}
}
}
}
return(ngram_dictionary)
}
to_df <- function(ngram) {
df <- data.frame(keys = names(ngram), values = values(ngram),
row.names = NULL)
return(df)
}
token_blogs <- tokenize_text(blogs)
token_news <- tokenize_text(news)
token_twitter <- tokenize_text(twitter)
unigram_blog <- count_ngrams(token_blogs, n=1)
unigram_news <- count_ngrams(token_news, n=1)
unigram_twitter <- count_ngrams(token_twitter, n=1)
bigram_blog <- count_ngrams(token_blogs, n=2)
bigram_news <- count_ngrams(token_news, n=2)
bigram_twitter <- count_ngrams(token_twitter, n=2)
unigram_blog_df <- to_df(unigram_blog)
unigram_news_df <- to_df(unigram_news)
unigram_twitter_df <- to_df(unigram_twitter)
bigram_blog_df <- to_df(bigram_blog)
bigram_news_df <- to_df(bigram_news)
bigram_twitter_df <- to_df(bigram_twitter)
row_names <- c("blog", "news", "twitter")
unigram_summary <- sapply(list(unigram_blog_df$values, unigram_news_df$values,
unigram_twitter_df$values), summary)
bigram_summary <- sapply(list(bigram_blog_df$values, bigram_news_df$values,
bigram_twitter_df$values), summary)
unigram_summary <- t(unigram_summary)
bigram_summary <- t(bigram_summary)
row.names(unigram_summary) <- row_names
row.names(bigram_summary) <- row_names
kable(unigram_summary, caption = "unigrams")
kable(bigram_summary, caption = "bigrams")
ngram_df <- function(df, n) {
top10_ngram_df <- df %>%
top_n(10, values) %>%
mutate(keys = sapply(keys, toString)) %>%
mutate(ngram = n) %>%
mutate(freq = values / sum(df$values) * 100)
return(top10_ngram_df)
}
unigram_blog <- ngram_df(unigram_blog_df, 1) %>%
mutate(source = "blogs")
unigram_news <- ngram_df(unigram_news_df, 1) %>%
mutate(source = "news")
unigram_twitter <- ngram_df(unigram_twitter_df, 1) %>%
mutate(source = "twitter")
bigram_blog <- ngram_df(bigram_blog_df, 2) %>%
mutate(source = "blogs")
bigram_news <- ngram_df(bigram_news_df, 2) %>%
mutate(source = "news")
bigram_twitter <- ngram_df(bigram_twitter_df, 2) %>%
mutate(source = "twitter")
unigrams <- rbind(unigram_blog, unigram_news, unigram_twitter)
bigrams <- rbind(bigram_blog, bigram_news, bigram_twitter)
plot_ngrams <- function(ngram) {
ggplot(ngram, aes(x = factor(keys), y = freq)) +
geom_bar(stat = "identity") +
facet_grid(cols = vars(factor(source)), scales = "free") +
coord_flip() +
xlab("n-grams") + ylab("freq (%)")
}
plot_ngrams(unigrams)
plot_ngrams(bigrams)
?file
?readLines
?strsplit
?runif
runif(1)
runif(2)
?readLines
?tm_map
??tm_map
?strsplit
l = strsplit("this string will be split", " ")
l
l[[1]]
l[[1]][[1]]
l[[0]]
length(l)
?sapply
?ngrams
??ngrams
s <- "this is a string that will be split"
w <- strsplit(s, " ")
ngrams(w)
ngrams(w, n = 1)
library(tm)
ngrams(w, n = 1)
ngrams(w, n = 3)
ngrams(w, 3)
ngrams(w, 3L)
ngrams(w, 2)
ngrams(w, 1)
?hash
??hash
?cat
?filter
?names
source("code/preProcessing.R")
source("code/preprocessing.R")
getwd()
